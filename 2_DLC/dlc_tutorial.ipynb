{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "DeepLabCut Toolbox\n",
    "\n",
    "https://github.com/DeepLabCut/DeepLabCut\n",
    "\n",
    "This notebook demonstrates the necessary steps to use DeepLabCut for your own project. This shows the most simple code to do so, but many of the functions have additional features, so please check out the overview & the protocol paper!\n",
    "\n",
    "This notebook illustrates how to:\n",
    "\n",
    "    create a project\n",
    "    extract training frames\n",
    "    label the frames\n",
    "    plot the labeled images\n",
    "    create a training set\n",
    "    train a network\n",
    "    evaluate a network\n",
    "    analyze a novel video\n",
    "    create an automatically labeled video\n",
    "    plot the trajectories\n",
    "\n",
    "This notebook demonstrates the necessary steps to use DeepLabCut for your own project.\n",
    "\n",
    "This shows the most simple code to do so, but many of the functions have additional features, so please check out the overview & the protocol paper!\n",
    "\n",
    "Nath*, Mathis* et al.: Using DeepLabCut for markerless pose estimation during behavior across species. Nature Protocols, 2019.\n",
    "\n",
    "Paper: https://www.nature.com/articles/s41596-019-0176-0\n",
    "\n",
    "Pre-print: https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a new project\n",
    "\n",
    "It is always good idea to keep the projects separate if you want to use different networks to analze your data. You should use one project if you are tracking similar subjects/items even if in different environments. This function creates a new project with sub-directories and a basic configuration file in the user defined directory otherwise the project is created in the current working directory.\n",
    "\n",
    "You can always add new videos (for lableing more data) to the project at any stage of the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 2.3.0...\n"
     ]
    }
   ],
   "source": [
    "# Primero, importamos el programa deeplabcut\n",
    "import deeplabcut\n",
    "# Si todo sale bien, el output deberÃ­a decir Loading DLC 2.3.0 o algo similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created \"C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\"\n",
      "Created \"C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\labeled-data\"\n",
      "Created \"C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\training-datasets\"\n",
      "Created \"C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\dlc-models\"\n",
      "15  videos from the directory C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\01_escuela\\01_resultados\\01__fluoxetina_grupo_1\\12_campo_abierto\\videos_convertidos were added to the project.\n",
      "Copying the videos\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 1.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 10.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 11.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 12.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 13.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 14.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 15.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 2.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 3.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 4.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 5.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 6.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 7.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 8.mp4\n",
      "C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 9.mp4\n",
      "Generated \"C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\config.yaml\"\n",
      "\n",
      "A new project with name Reaching-Santi-2023-01-24 is created at C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n",
      " Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n",
      ". [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage).\n"
     ]
    }
   ],
   "source": [
    "task='Reaching' # Enter the name of your experiment Task\n",
    "experimenter='Santi' # Enter the name of the experimenter\n",
    "video=[r'C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\01_escuela\\01_resultados\\01__fluoxetina_grupo_1\\12_campo_abierto\\videos_convertidos'] # Enter the paths of your videos OR FOLDER you want to grab frames from.\n",
    "\n",
    "path_config_file=deeplabcut.create_new_project(task,experimenter,video,copy_videos=True) \n",
    "\n",
    "# NOTE: The function returns the path, where your project is. \n",
    "# You could also enter this manually (e.g. if the project is already created and you want to pick up, where you stopped...)\n",
    "#path_config_file = '/home/Mackenzie/Reaching/config.yaml' # Enter the path of the config file that was just created from the above step (check the folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, go edit the config.yaml file that was created!\n",
    "\n",
    "Add your body part labels, edit the number of frames to extract per video, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mdeeplabcut\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'automatic'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'kmeans'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0muserfeedback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcluster_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcluster_resizewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcluster_color\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mopencv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mslider_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconfig3d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mextracted_cam\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvideos_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Extracts frames from the project videos.\n",
      "\n",
      "Frames will be extracted from videos listed in the config.yaml file.\n",
      "\n",
      "The frames are selected from the videos in a randomly and temporally uniformly\n",
      "distributed way (``uniform``), by clustering based on visual appearance\n",
      "(``k-means``), or by manual selection.\n",
      "\n",
      "After frames have been extracted from all videos from one camera, matched frames\n",
      "from other cameras can be extracted using ``mode = \"match\"``. This is necessary if\n",
      "you plan to use epipolar lines to improve labeling across multiple camera angles.\n",
      "It will overwrite previously extracted images from the second camera angle if\n",
      "necessary.\n",
      "\n",
      "Please refer to the user guide for more details on methods and parameters\n",
      "https://www.nature.com/articles/s41596-019-0176-0 or the preprint:\n",
      "https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "config : string\n",
      "    Full path of the config.yaml file as a string.\n",
      "\n",
      "mode : string. Either ``\"automatic\"``, ``\"manual\"`` or ``\"match\"``.\n",
      "    String containing the mode of extraction. It must be either ``\"automatic\"`` or\n",
      "    ``\"manual\"`` to extract the initial set of frames. It can also be ``\"match\"``\n",
      "    to match frames between the cameras in preparation for the use of epipolar line\n",
      "    during labeling; namely, extract from camera_1 first, then run this to extract\n",
      "    the matched frames in camera_2.\n",
      "\n",
      "    WARNING: if you use ``\"match\"``, and you previously extracted and labeled\n",
      "    frames from the second camera, this will overwrite your data. This will require\n",
      "    you to delete the ``collectdata(.h5/.csv)`` files before labeling. Use with\n",
      "    caution!\n",
      "\n",
      "algo : string, Either ``\"kmeans\"`` or ``\"uniform\"``, Default: `\"kmeans\"`.\n",
      "    String specifying the algorithm to use for selecting the frames. Currently,\n",
      "    deeplabcut supports either ``kmeans`` or ``uniform`` based selection. This flag\n",
      "    is only required for ``automatic`` mode and the default is ``kmeans``. For\n",
      "    ``\"uniform\"``, frames are picked in temporally uniform way, ``\"kmeans\"``\n",
      "    performs clustering on downsampled frames (see user guide for details).\n",
      "\n",
      "    NOTE: Color information is discarded for ``\"kmeans\"``, thus e.g. for\n",
      "    camouflaged octopus clustering one might want to change this.\n",
      "\n",
      "crop : bool or str, optional\n",
      "    If ``True``, video frames are cropped according to the corresponding\n",
      "    coordinates stored in the project configuration file. Alternatively, if\n",
      "    cropping coordinates are not known yet, crop=``\"GUI\"`` triggers a user\n",
      "    interface where the cropping area can be manually drawn and saved.\n",
      "\n",
      "userfeedback: bool, optional\n",
      "    If this is set to ``False`` during ``\"automatic\"`` mode then frames for all\n",
      "    videos are extracted. The user can set this to ``\"True\"``, which will result in\n",
      "    a dialog, where the user is asked for each video if (additional/any) frames\n",
      "    from this video should be extracted. Use this, e.g. if you have already labeled\n",
      "    some folders and want to extract data for new videos.\n",
      "\n",
      "cluster_resizewidth: int, default: 30\n",
      "    For ``\"k-means\"`` one can change the width to which the images are downsampled\n",
      "    (aspect ratio is fixed).\n",
      "\n",
      "cluster_step: int, default: 1\n",
      "    By default each frame is used for clustering, but for long videos one could\n",
      "    only use every nth frame (set using this parameter). This saves memory before\n",
      "    clustering can start, however, reading the individual frames takes longer due\n",
      "    to the skipping.\n",
      "\n",
      "cluster_color: bool, default: False\n",
      "    If ``\"False\"`` then each downsampled image is treated as a grayscale vector\n",
      "    (discarding color information). If ``\"True\"``, then the color channels are\n",
      "    considered. This increases the computational complexity.\n",
      "\n",
      "opencv: bool, default: True\n",
      "    Uses openCV for loading & extractiong (otherwise moviepy (legacy)).\n",
      "\n",
      "slider_width: int, default: 25\n",
      "    Width of the video frames slider, in percent of window.\n",
      "\n",
      "config3d: string, optional\n",
      "    Path to the project configuration file in the 3D project. This will be used to\n",
      "    match frames extracted from all cameras present in the field 'camera_names' to\n",
      "    the frames extracted from the camera given by the parameter 'extracted_cam'.\n",
      "\n",
      "extracted_cam: int, default: 0\n",
      "    The index of the camera that already has extracted frames. This will match\n",
      "    frame numbers to extract for all other cameras. This parameter is necessary if\n",
      "    you wish to use epipolar lines in the labeling toolbox. Only use if\n",
      "    ``mode='match'`` and ``config3d`` is provided.\n",
      "\n",
      "videos_list: list[str], Default: None\n",
      "    A list of the string containing full paths to videos to extract frames for. If\n",
      "    this is left as ``None`` all videos specified in the config file will have\n",
      "    frames extracted. Otherwise one can select a subset by passing those paths.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "None\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Use the function ``add_new_videos`` at any stage of the project to add new videos\n",
      "to the config file and extract their frames.\n",
      "\n",
      "The following parameters for automatic extraction are used from the config file\n",
      "\n",
      "* ``numframes2pick``\n",
      "* ``start`` and ``stop``\n",
      "\n",
      "While selecting the frames manually, you do not need to specify the ``crop``\n",
      "parameter in the command. Rather, you will get a prompt in the graphic user\n",
      "interface to choose if you need to crop or not.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "To extract frames automatically with 'kmeans' and then crop the frames\n",
      "\n",
      ">>> deeplabcut.extract_frames(\n",
      "        config='/analysis/project/reaching-task/config.yaml',\n",
      "        mode='automatic',\n",
      "        algo='kmeans',\n",
      "        crop=True,\n",
      "    )\n",
      "\n",
      "To extract frames automatically with 'kmeans' and then defining the cropping area\n",
      "using a GUI\n",
      "\n",
      ">>> deeplabcut.extract_frames(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        'automatic',\n",
      "        'kmeans',\n",
      "        'GUI',\n",
      "    )\n",
      "\n",
      "To consider the color information when extracting frames automatically with\n",
      "'kmeans'\n",
      "\n",
      ">>> deeplabcut.extract_frames(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        'automatic',\n",
      "        'kmeans',\n",
      "        cluster_color=True,\n",
      "    )\n",
      "\n",
      "To extract frames automatically with 'uniform' and then crop the frames\n",
      "\n",
      ">>> deeplabcut.extract_frames(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        'automatic',\n",
      "        'uniform',\n",
      "        crop=True,\n",
      "    )\n",
      "\n",
      "To extract frames manually\n",
      "\n",
      ">>> deeplabcut.extract_frames(\n",
      "        '/analysis/project/reaching-task/config.yaml', 'manual'\n",
      "    )\n",
      "\n",
      "To extract frames manually, with a 60% wide frames slider\n",
      "\n",
      ">>> deeplabcut.extract_frames(\n",
      "        '/analysis/project/reaching-task/config.yaml', 'manual', slider_width=60,\n",
      "    )\n",
      "\n",
      "To extract frames from a second camera that match the frames extracted from the\n",
      "first\n",
      "\n",
      ">>> deeplabcut.extract_frames(\n",
      "        '/analysis/project/reaching-task/config.yaml',\n",
      "        mode='match',\n",
      "        extracted_cam=0,\n",
      "    )\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\santi\\anaconda3\\envs\\deeplabcut\\lib\\site-packages\\deeplabcut\\generate_training_dataset\\frame_extraction.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note that you can see more information about ANY function by adding a ? at the end, i.e.\n",
    "\n",
    "deeplabcut.extract_frames?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Extract frames from videos\n",
    "\n",
    "A key point for a successful feature detector is to select diverse frames, which are typical for the behavior you study that should be labeled.\n",
    "\n",
    "This function selects N frames either uniformly sampled from a particular video (or folder) ('uniform'). Note: this might not yield diverse frames, if the behavior is sparsely distributed (consider using kmeans), and/or select frames manually etc.\n",
    "\n",
    "Also make sure to get select data from different (behavioral) sessions and different animals if those vary substantially (to train an invariant feature detector).\n",
    "\n",
    "Individual images should not be too big (i.e. < 850 x 850 pixel). Although this can be taken care of later as well, it is advisable to crop the frames, to remove unnecessary parts of the frame as much as possible.\n",
    "\n",
    "Always check the output of cropping. If you are happy with the results proceed to labeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file read successfully.\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 1.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 599.94  seconds.\n",
      "Extracting and downsampling... 10415  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10415it [00:16, 634.59it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 10.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 600.0  seconds.\n",
      "Extracting and downsampling... 10404  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10404it [00:14, 698.79it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 11.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 600.12  seconds.\n",
      "Extracting and downsampling... 10382  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10382it [00:15, 661.03it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 12.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 599.83  seconds.\n",
      "Extracting and downsampling... 10401  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10401it [00:15, 672.13it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 13.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 600.17  seconds.\n",
      "Extracting and downsampling... 10395  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10395it [00:16, 638.07it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 14.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 599.94  seconds.\n",
      "Extracting and downsampling... 10397  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10397it [00:17, 609.87it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 15.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 600.0  seconds.\n",
      "Extracting and downsampling... 10410  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10410it [00:18, 577.45it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 2.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 600.06  seconds.\n",
      "Extracting and downsampling... 10405  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10405it [00:17, 592.98it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 3.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 599.88  seconds.\n",
      "Extracting and downsampling... 10390  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10390it [00:15, 679.82it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 4.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 599.94  seconds.\n",
      "Extracting and downsampling... 10403  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10403it [00:14, 700.30it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 5.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 599.83  seconds.\n",
      "Extracting and downsampling... 10395  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10395it [00:16, 635.23it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 6.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 600.0  seconds.\n",
      "Extracting and downsampling... 10422  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10422it [00:16, 640.22it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 7.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 599.94  seconds.\n",
      "Extracting and downsampling... 10415  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10415it [00:16, 626.72it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 8.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 599.83  seconds.\n",
      "Extracting and downsampling... 10407  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10407it [00:18, 572.39it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Do you want to extract (perhaps additional) frames for video: C:\\Users\\Santi\\OneDrive - ciencias.unam.mx\\31_git\\maestria\\maestria\\2_DLC\\Reaching-Santi-2023-01-24\\videos\\Test 9.mp4 ?\n",
      "Extracting frames based on kmeans ...\n",
      "Kmeans-quantization based extracting of frames from 0.0  seconds to 600.12  seconds.\n",
      "Extracting and downsampling... 10418  frames from the video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10418it [00:17, 602.17it/s]\n",
      "c:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans clustering ... (this might take a while)\n",
      "Frames were successfully extracted, for the videos listed in the config.yaml file.\n",
      "\n",
      "You can now label the frames using the function 'label_frames' (Note, you should label frames extracted from diverse videos (and many videos; we do not recommend training on single videos!)).\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#there are other ways to grab frames, such as uniformly; please see the paper:\n",
    "\n",
    "#AUTOMATIC:\n",
    "deeplabcut.extract_frames(path_config_file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AND/OR:\n",
    "#SELECT RARE EVENTS MANUALLY:\n",
    "%gui wx\n",
    "deeplabcut.extract_frames(path_config_file,'manual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Label the extracted frames\n",
    "\n",
    "Only videos in the config file can be used to extract the frames. Extracted labels for each video are stored in the project directory under the subdirectory 'labeled-data'. Each subdirectory is named after the name of the video. The toolbox has a labeling toolbox which could be used for labeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot activate multiple GUI eventloops",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mgui\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwx\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m deeplabcut\u001b[39m.\u001b[39;49mlabel_frames(path_config_file)\n",
      "File \u001b[1;32mc:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\gui\\tabs\\label_frames.py:19\u001b[0m, in \u001b[0;36mlabel_frames\u001b[1;34m(config_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlabel_frames\u001b[39m(config_path):\n\u001b[1;32m---> 19\u001b[0m     _ \u001b[39m=\u001b[39m launch_napari(config_path)\n",
      "File \u001b[1;32mc:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\deeplabcut\\gui\\widgets.py:38\u001b[0m, in \u001b[0;36mlaunch_napari\u001b[1;34m(files)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlaunch_napari\u001b[39m(files\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 38\u001b[0m     viewer \u001b[39m=\u001b[39m napari\u001b[39m.\u001b[39;49mViewer()\n\u001b[0;32m     39\u001b[0m     \u001b[39m# Automatically activate the napari-deeplabcut plugin\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m viewer\u001b[39m.\u001b[39mwindow\u001b[39m.\u001b[39mplugins_menu\u001b[39m.\u001b[39mactions():\n",
      "File \u001b[1;32mc:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\napari\\viewer.py:67\u001b[0m, in \u001b[0;36mViewer.__init__\u001b[1;34m(self, title, ndisplay, order, axis_labels, show)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mwindow\u001b[39;00m \u001b[39mimport\u001b[39;00m Window\n\u001b[0;32m     65\u001b[0m _initialize_plugins()\n\u001b[1;32m---> 67\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_window \u001b[39m=\u001b[39m Window(\u001b[39mself\u001b[39;49m, show\u001b[39m=\u001b[39;49mshow)\n\u001b[0;32m     68\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_instances\u001b[39m.\u001b[39madd(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\napari\\_qt\\qt_main_window.py:454\u001b[0m, in \u001b[0;36mWindow.__init__\u001b[1;34m(self, viewer, show)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, viewer: \u001b[39m'\u001b[39m\u001b[39mViewer\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m*\u001b[39m, show: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    453\u001b[0m     \u001b[39m# create QApplication if it doesn't already exist\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m     get_app()\n\u001b[0;32m    456\u001b[0m     \u001b[39m# Dictionary holding dock widgets\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dock_widgets: Dict[\n\u001b[0;32m    458\u001b[0m         \u001b[39mstr\u001b[39m, QtViewerDockWidget\n\u001b[0;32m    459\u001b[0m     ] \u001b[39m=\u001b[39m WeakValueDictionary()\n",
      "File \u001b[1;32mc:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\napari\\_qt\\qt_event_loop.py:182\u001b[0m, in \u001b[0;36mget_app\u001b[1;34m(app_name, app_version, icon, org_name, org_domain, app_id, ipy_interactive)\u001b[0m\n\u001b[0;32m    180\u001b[0m     ipy_interactive \u001b[39m=\u001b[39m get_settings()\u001b[39m.\u001b[39mapplication\u001b[39m.\u001b[39mipy_interactive\n\u001b[0;32m    181\u001b[0m \u001b[39mif\u001b[39;00m _IPYTHON_WAS_HERE_FIRST:\n\u001b[1;32m--> 182\u001b[0m     _try_enable_ipython_gui(\u001b[39m'\u001b[39;49m\u001b[39mqt\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m ipy_interactive \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _ipython_has_eventloop():\n\u001b[0;32m    185\u001b[0m     notification_manager\u001b[39m.\u001b[39mnotification_ready\u001b[39m.\u001b[39mconnect(\n\u001b[0;32m    186\u001b[0m         NapariQtNotification\u001b[39m.\u001b[39mshow_notification\n\u001b[0;32m    187\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\napari\\_qt\\qt_event_loop.py:347\u001b[0m, in \u001b[0;36m_try_enable_ipython_gui\u001b[1;34m(gui)\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39mif\u001b[39;00m shell\u001b[39m.\u001b[39mactive_eventloop \u001b[39m!=\u001b[39m gui:\n\u001b[1;32m--> 347\u001b[0m     shell\u001b[39m.\u001b[39;49menable_gui(gui)\n",
      "File \u001b[1;32mc:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\zmqshell.py:481\u001b[0m, in \u001b[0;36mZMQInteractiveShell.enable_gui\u001b[1;34m(self, gui)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39meventloops\u001b[39;00m \u001b[39mimport\u001b[39;00m enable_gui \u001b[39mas\u001b[39;00m real_enable_gui\n\u001b[0;32m    480\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 481\u001b[0m     real_enable_gui(gui)\n\u001b[0;32m    482\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactive_eventloop \u001b[39m=\u001b[39m gui\n\u001b[0;32m    483\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Santi\\anaconda3\\envs\\DEEPLABCUT\\lib\\site-packages\\ipykernel\\eventloops.py:576\u001b[0m, in \u001b[0;36menable_gui\u001b[1;34m(gui, kernel)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[39mif\u001b[39;00m loop \u001b[39mand\u001b[39;00m kernel\u001b[39m.\u001b[39meventloop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m kernel\u001b[39m.\u001b[39meventloop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m loop:\n\u001b[0;32m    575\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot activate multiple GUI eventloops\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 576\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    577\u001b[0m kernel\u001b[39m.\u001b[39meventloop \u001b[39m=\u001b[39m loop\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot activate multiple GUI eventloops"
     ]
    }
   ],
   "source": [
    "%gui wx\n",
    "deeplabcut.label_frames(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Check the labels\n",
    "\n",
    "[OPTIONAL] Checking if the labels were created and stored correctly is beneficial for training, since labeling is one of the most critical parts for creating the training dataset. The DeepLabCut toolbox provides a function `check_labels' to do so. It is used as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.check_labels(path_config_file) #this creates a subdirectory with the frames + your labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create a training dataset\n",
    "\n",
    "This function generates the training data information for network training based on the pandas dataframes that hold label information. The user can set the fraction of the training set size (from all labeled image in the hd5 file) in the config.yaml file. While creating the dataset, the user can create multiple shuffles if they want to benchmark the performance (typcailly, 1 is what you will set, so you pass nothing!).\n",
    "\n",
    "After running this script the training dataset is created and saved in the project directory under the subdirectory 'training-datasets'\n",
    "\n",
    "This function also creates new subdirectories under dlc-models and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as pose_cfg.yaml. For most all use cases we have seen, the defaults are perfectly fine.\n",
    "\n",
    "Now it is the time to start training the network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)\n",
    "#remember, there are several networks you can pick, the default is resnet-50!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Start training:\n",
    "\n",
    "This function trains the network for a specific shuffle of the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.train_network(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Start evaluating\n",
    "\n",
    "This function evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images) and stores the results as .csv file in a subdirectory under evaluation-results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(path_config_file, plotting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Start Analyzing videos\n",
    "\n",
    "This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable snapshotindex in the config.yaml file. Otherwise, by default the most recent snapshot is used to analyse the video.\n",
    "\n",
    "The results are stored in hd5 file in the same directory where the video resides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videofile_path = ['videos/video3.avi','videos/video4.avi'] #Enter a folder OR a list of videos to analyze.\n",
    "\n",
    "deeplabcut.analyze_videos(path_config_file,videofile_path, videotype='.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Extract outlier frames [optional step]\n",
    "\n",
    "This is an optional step and is used only when the evaluation results are poor i.e. the labels are incorrectly predicted. In such a case, the user can use the following function to extract frames where the labels are incorrectly predicted. This step has many options, so please look at:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "deeplabcut.extract_outlier_frames?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "deeplabcut.extract_outlier_frames(path_config_file,['/videos/video3.avi']) #pass a specific video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEEPLABCUT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9f1b96c7cbec1a23f5bf205e05c705f40a6edbb816553a9b68ce9f1a1137f40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
